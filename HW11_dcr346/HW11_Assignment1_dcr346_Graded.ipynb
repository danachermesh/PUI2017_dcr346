{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Time series clustering exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the census bureau business data for all years 1993-2014. You can investigate using the API (I have not done it with the census bureau). I did is as you see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#these commands can be run on the shell and get the data with the command wget\n",
    "#the cell needs to be run only once\n",
    "#!for ((y=93; y<=99; y+=1)); do wget \\\n",
    "#ftp://ftp.census.gov/Econ2001_And_Earlier/CBP_CSV/zbp$y\\totals.zip; done\n",
    "\n",
    "#!for ((y=0; y<=1; y+=1)); do wget \\\n",
    "#ftp://ftp.census.gov/Econ2001_And_Earlier/CBP_CSV/zbp0$y\\totals.zip; done\n",
    "\n",
    "#!for ((y=2; y<=9; y+=1)); do wget \\\n",
    "#ftp://ftp.census.gov/econ200$y\\/CBP_CSV/zbp0$y\\totals.zip; done\n",
    "\n",
    "#!for ((y=10; y<=14; y+=1)); do wget \\\n",
    "#ftp://ftp.census.gov/econ20$y\\/CBP_CSV/zbp$y\\totals.zip; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T07:06:43.156591",
     "start_time": "2017-12-07T07:06:41.245257"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    " Download the NYC zipcodes shapefile. One of many ways in which you can get the zipcodes shapefile for NYC\n",
    " https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## TASKS:\n",
    "    \n",
    "    1. get and prep your data.\n",
    "    2. cluster the NUMBER OF ESTABLISHMENTS time series with K-means in **a few** clusters (as discussed there is no real good, sound way to decide what a good number is here. try a few options, keeping in mind a few is more than a couple, but i recommand you stay within the single digit numbers)\n",
    "    3. plot the cluster centers (if you used K means those are the means of the clusters). you can plot for example the cluster centers overlayed on each time series (using the alpha channel to control the opacity in the plot may be helpful here).\n",
    "    4. Use another clustering algorithm (of your choice)\n",
    "    5. overlay your data on a NYC map: you can use shapefiles for the zip codes and different colors for different clusters\n",
    "    6. Compare the results of the 2 algorithms\n",
    "    7. attempt an interpretation. this is dangerous ground: clustering is an exploratory tool so you do not want to jump to conclusions because you see some clusters! but seeing structure in your data can inform your next moves as an investigator. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/rh/anaconda/root/envs/PUI2016_Python3/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import shapely\n",
    "import json\n",
    "import requests \n",
    "import urllib\n",
    "import zipfile \n",
    "\n",
    "from scipy.cluster.vq import kmeans2, whiten\n",
    "from sklearn import preprocessing\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. get and prep your data\n",
    "## Read in NYC Zipcode shp as GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cusp/vmr286/PUIdata\n"
     ]
    }
   ],
   "source": [
    "PUIDATA = os.getenv('PUIDATA')\n",
    "print(PUIDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>BLDGZIP</th>\n",
       "      <th>PO_NAME</th>\n",
       "      <th>POPULATION</th>\n",
       "      <th>AREA</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>ST_FIPS</th>\n",
       "      <th>CTY_FIPS</th>\n",
       "      <th>URL</th>\n",
       "      <th>SHAPE_AREA</th>\n",
       "      <th>SHAPE_LEN</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>0</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>18681.0</td>\n",
       "      <td>2.269930e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Queens</td>\n",
       "      <td>36</td>\n",
       "      <td>081</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((1038098.251871482 188138.3800067157,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>62426.0</td>\n",
       "      <td>2.963100e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((1001613.712964058 186926.4395172149,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>83866.0</td>\n",
       "      <td>4.197210e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((1011174.275535807 183696.33770971, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>56527.0</td>\n",
       "      <td>2.369863e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((995908.3654508889 183617.6128015518,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>72280.0</td>\n",
       "      <td>3.686880e+07</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>36</td>\n",
       "      <td>047</td>\n",
       "      <td>http://www.usps.com/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLYGON ((991997.1134308875 176307.4958601296,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ZIPCODE BLDGZIP   PO_NAME  POPULATION          AREA STATE  COUNTY ST_FIPS  \\\n",
       "0   11436       0   Jamaica     18681.0  2.269930e+07    NY  Queens      36   \n",
       "1   11213       0  Brooklyn     62426.0  2.963100e+07    NY   Kings      36   \n",
       "2   11212       0  Brooklyn     83866.0  4.197210e+07    NY   Kings      36   \n",
       "3   11225       0  Brooklyn     56527.0  2.369863e+07    NY   Kings      36   \n",
       "4   11218       0  Brooklyn     72280.0  3.686880e+07    NY   Kings      36   \n",
       "\n",
       "  CTY_FIPS                   URL  SHAPE_AREA  SHAPE_LEN  \\\n",
       "0      081  http://www.usps.com/         0.0        0.0   \n",
       "1      047  http://www.usps.com/         0.0        0.0   \n",
       "2      047  http://www.usps.com/         0.0        0.0   \n",
       "3      047  http://www.usps.com/         0.0        0.0   \n",
       "4      047  http://www.usps.com/         0.0        0.0   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((1038098.251871482 188138.3800067157,...  \n",
       "1  POLYGON ((1001613.712964058 186926.4395172149,...  \n",
       "2  POLYGON ((1011174.275535807 183696.33770971, 1...  \n",
       "3  POLYGON ((995908.3654508889 183617.6128015518,...  \n",
       "4  POLYGON ((991997.1134308875 176307.4958601296,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://data.cityofnewyork.us/download/i8iw-xf4u/application%2Fzip'\n",
    "\n",
    "urllib.request.urlretrieve(url, 'nyc.zip')\n",
    "\n",
    "os.system(\"unzip -d %s puma.zip\"%PUIDATA)\n",
    "\n",
    "\n",
    "zipshp = gpd.GeoDataFrame.from_file(PUIDATA + '/ZIP_CODE_040114.shp')\n",
    "zipshp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## You can use zipfile module in python to unzip the files\n",
    "it should be install in your system, but if it is not you can get the code with wget from here\n",
    "https://github.com/python/cpython/blob/2.7/Lib/zipfile.py\n",
    "remembering to use the raw link\n",
    "(or you can use the usual shell commands, and miss the chance to learn something new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZIPCODE         int64\n",
       "BLDGZIP        object\n",
       "PO_NAME        object\n",
       "POPULATION    float64\n",
       "AREA          float64\n",
       "STATE          object\n",
       "COUNTY         object\n",
       "ST_FIPS        object\n",
       "CTY_FIPS       object\n",
       "URL            object\n",
       "SHAPE_AREA    float64\n",
       "SHAPE_LEN     float64\n",
       "geometry       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipshp['ZIPCODE'] = zipshp['ZIPCODE'].astype(int)\n",
    "zipshp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'zbp94totals.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6ae7e4b43c6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m94\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zbp'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'totals.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mzf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/rh/anaconda/root/envs/PUI2016_Python3/lib/python3.5/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'zbp94totals.zip'"
     ]
    }
   ],
   "source": [
    "est = []\n",
    "\n",
    "NYCzip = zipshp.ZIPCODE.unique()\n",
    "\n",
    "for i in range(94,100):\n",
    "    fname = 'zbp' + str(i) + 'totals.zip'\n",
    "    zf = zipfile.ZipFile(fname)\n",
    "    df = pd.read_csv(zf.open(fname.replace('.zip','.txt')))\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df = (df[(df['zip'].astype(str).str.startswith('1'))])\n",
    "    df = df[['zip','name','est']]\n",
    "    df['year'] = i\n",
    "    est.append(df)\n",
    "\n",
    "for i in range(0,15):\n",
    "    i = str(i).zfill(2)\n",
    "    fname = 'zbp' + str(i) + 'totals.zip'\n",
    "    zf = zipfile.ZipFile(fname)\n",
    "    df = pd.read_csv(zf.open(fname.replace('.zip','.txt')))\n",
    "    df.columns = [x.lower() for x in df.columns]\n",
    "    df = (df[(df['zip'].astype(str).str.startswith('1'))])\n",
    "    df = df[['zip','name','est']]\n",
    "    df['year'] = i\n",
    "    est.append(df)\n",
    "    \n",
    "est = pd.concat(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est[::3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge GeoDataFrame with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estshp = zipshp.merge(est, left_on='ZIPCODE', right_on='zip')\n",
    "estshp.drop(['STATE','BLDGZIP','URL','zip','name'], inplace=True, axis=1)\n",
    "estshp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(estshp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estshp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "est_ts = pd.pivot_table(estshp, values='est', index=['ZIPCODE'], columns='year')\n",
    "\n",
    "est_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(est_ts.columns)\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "you may need to clean your data: for some NYC zip codes there may be no info\n",
    "sanity check: you should have 20 (N_timestamps) datapoints per time series and about 250 zipcodes (Nzipcodes)\n",
    "\n",
    "\n",
    "IMPORTANT: we talked about the importance of \"whitening\" your data.\n",
    "Whitenings decorrelates the data: it makes the features independent so that the data covariance matrix is the identity matrix.\n",
    "Whitening your data in time series analysis is in most cases **wrong**: you are modifying your time behaviour. This is because of the strong correlation between features (two consecutive time stamps for the same observation, the same zip code here, are strongly correlated). Here instead you want to standardize your time series: subtract the mean and divide each time series (separately) by its standard deviation. As a sanity check (if you use skitlearn Kmeans or skitlearns kmeans2): you want your data array to be shaped Nzipcodes x Ntimestamps\n",
    "\n",
    "mydata.shape should be (Nzipcodes, Ntimestamps)\n",
    "\n",
    "mydata[i].std() shoould be 1 for all i in range(len(Nzipcodes))\n",
    "\n",
    "mydata[i].mean() should be ~0 for all i in range(len(Nzipcodes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_ts = est_ts.dropna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYCzip = est_ts.index\n",
    "type(NYCzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standardize the time series\n",
    "est_scaled = preprocessing.scale(est_ts)\n",
    "\n",
    "# verifing the std of the standardized data is 1\n",
    "est_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_scaled = pd.DataFrame(est_scaled)\n",
    "est_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_scaled.set_index(NYCzip, inplace=True)\n",
    "est_scaled.columns = years\n",
    "\n",
    "est_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. cluster the NUMBER OF ESTABLISHMENTS time series with K-means in **a few** clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define + run Elbow function to detect the number of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from Applied Data Sceince lab, NYU CUSP, Fall 2017\n",
    "\n",
    "def elbow(data,K):\n",
    "#data is your input as numpy form\n",
    "#K is a list of number of clusters you would like to show.\n",
    "    # Run the KMeans model and save all the results for each number of clusters\n",
    "    KM = [KMeans(n_clusters=k).fit(data) for k in K]\n",
    "    \n",
    "    # Save the centroids for each model with a increasing k\n",
    "    centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "    # For each k, get the distance between the data with each center. \n",
    "    D_k = [cdist(data, cent, 'euclidean') for cent in centroids]\n",
    "    \n",
    "    # But we only need the distance to the nearest centroid since we only calculate dist(x,ci) for its own cluster.\n",
    "    globals()['dist'] = [np.min(D,axis=1) for D in D_k]\n",
    "    \n",
    "    # Calculate the Average SSE.\n",
    "    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n",
    "    \n",
    "    \n",
    "    # elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Total with-in sum of square plot. Another way to show the result.\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(data)**2)/data.shape[0]\n",
    "    bss = tss-wcss\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, bss/tss*100, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Percentage of variance explained')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is taken from Applied Data Sceince lab, NYU CUSP, Fall 2017\n",
    "\n",
    "X=np.asarray(est_scaled.iloc[:,:-1])\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=324)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "#elbow\n",
    "elbow(X,range(1,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.1: elbow function to detect number of clusters.\n",
    "According to the elbow, I will use _5 clasters_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning each timeseries to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = KMeans(n_clusters=5, random_state=324)\n",
    "cluster_labels = clusters.fit_predict(X)\n",
    "est_scaled['cluster_labels'] = cluster_labels\n",
    "\n",
    "est_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = est_scaled[['cluster_labels']]\n",
    "cluster.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. plot the cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermean = est_scaled.groupby('cluster_labels').mean()\n",
    "clustermean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(12,8))\n",
    "pl.plot(clustermean.ix[:,5:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use another clustering algorithm >>\n",
    "# DBScan clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=0.8, min_samples=2).fit(cluster)\n",
    "labels = (db.labels_).astype(int)\n",
    "num_clusters = len(set(labels)) - (1 if -1 in labels else 0) # the last statements removes outliers\n",
    "clusters = pd.Series([cluster[labels == i] for i in range(num_clusters)])\n",
    "print('Number of clusters: %d' % num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=0.8, min_samples=2).fit(est_scaled)\n",
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the centroid from DBscan as mean of cluster members\n",
    "# the cluster center is the mean of the cluster for both K Means and DBScan\n",
    "def getCentroid(points):\n",
    "    #print points[:,0], np.nanmean(points[:,0])\n",
    "    return np.nanmean(points[:,0]), np.nanmean(points[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot DBScan centroids\n",
    "centroids = np.zeros((num_clusters,2))\n",
    "\n",
    "for i in labels:\n",
    "    centroids[i] = getCentroid(cluster[labels == i])\n",
    "colorsc2 = get_colors(np.arange(num_clusters), pl.cm.jet)\n",
    "\n",
    "pl.figure(figsize=(10, 6), dpi=100)\n",
    "for i in np.unique(labels):\n",
    "    if int(i) == -1:\n",
    "        continue\n",
    "    pl.scatter(centroids[i,0], centroids[i,1], c=colorsc2[i], alpha=.3, \n",
    "                s=sum(labels == i)*100, \n",
    "                label=[\"%i\"%i])\n",
    "#pl.legend(numpoints=1, scatterpoints=1)\n",
    "colors2 = get_colors(labels, pl.cm.jet)\n",
    "\n",
    "pl.scatter(cluster[:,0], cluster[:,1], c=colors2, s=20)\n",
    "pl.scatter(cluster[:,0][labels==-1], cluster[:,1][labels==-1], c='k', alpha=0.4, s=300)\n",
    "pl.xlabel(\"longitude (deg)\", fontsize=18)\n",
    "pl.ylabel(\"latitude(deg)\", fontsize=18)\n",
    "pl.title(\"DBscan clustering, %d culsters, %d outliers\"%(num_clusters,\n",
    "                                                        sum(labels == -1)), fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Overlay your data on a NYC map\n",
    "## Merging w zipcode shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustershp = zipshp.merge(cluster, left_on='ZIPCODE', right_index=True)\n",
    "clustershp.drop(['PO_NAME','STATE','BLDGZIP','URL','ST_FIPS','CTY_FIPS'], inplace=True, axis=1)\n",
    "clustershp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustershp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustershp.plot(column='cluster_labels', figsize=(10,10), cmap='Purples', edgecolor=\"black\", lw=.2, legend=True, scheme='Fisher_Jenks')\n",
    "pl.title('5 Clusters of buisness establishments by Zipcode, 1994-2014', fontsize=20)\n",
    "pl.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig.2 Five Clusters of buisness establishments by Zipcode, 1994-2014\n",
    "Clusters are observed in south Brooklyn and center SI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = pl.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "# plt.clf()\n",
    "# Plot the training points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=cluster_labels.labels_, cmap=plt.cm.viridis, alpha=0.4)\n",
    "ax.set_xlabel('First principal component', fontsize=15)\n",
    "ax.set_ylabel('Second principal component', fontsize=15)\n",
    "ax.set_title('3 Clusters of buisness establishments by Zipcode, year 2016', fontsize=20)\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### The map of the clusters may look something like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T07:08:28.106869",
     "start_time": "2017-12-07T07:08:06.335255"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Figure 3: \n",
    "cloropleth of  cluster centers for 5 k-means clusters of business patterns (number of businesses) at the zipcode level for NYC zipcodes: each color indicates a cluster. The business pattern time series are plotted at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### or maybe like this, depending on which algorithm you use, and how you proceed to preprocess your data and how you cluster it. There is no one correct answer, but general trends should be retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T07:09:49.803126",
     "start_time": "2017-12-07T07:09:33.460426"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Figure 9: \n",
    "As figures 3, 5, 7 for hierarchical agglomerative clustering in 7 clusters, with smoothed time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### And if you use hierarchical clustering and make a dandrogram it may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-07T07:23:50.040050",
     "start_time": "2017-12-07T07:23:49.310680"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(\"dandrogram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Points: 4 - data not reproducible, all tasks not attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PUI2016_Python3",
   "language": "python",
   "name": "pui2016_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "514px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
